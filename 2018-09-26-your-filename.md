We use a lot of probability concepts in statistics and hence in machine learning, they are like using the same methodologies. In probability, the model is given and we need to predict the data. While in statistics we start with the data and predict the model. We look at probability and search from data distributions which closely match the data distribution that we have. Then we assume that the function or the model must be the same as the one we looked into in probability theory.


source
Conditional Probability and Bayes’ theorem

source
Conditional Probability is the study of the probability of two things happening together. The way to do this is by applying Bayes’ theorem which provides a simple way for calculating conditional probabilities.

Speaking mathematically, the probability of the model given the data is probability of the data given the model times the ratio of the independent probability of the model and the independent probability of the data.


Bayes’ theorem is simple but has profound implications. The degree of belief in a machine learning model can also be thought of as probabilities and machine learning can be thought of as learning models of data. Thus, we can consider multiple models, find out the probabilities they have given the data and then consider the model which has the higher probability. In practice, this may not be that simple, but at least it will be easier to understand and not fish on routes with zero probabilities.

If you are interested in talking more on this, just drop me a message @alt227Joydeep. I would be glad to discuss this further. Also please hit the claps and help this article reach more audience.

Reference:

•Wiki. Statistical data
•StackOverflow. Use of moments in statistics.
•Youtube. Moments.
•Wiki. Covariance
•A brief introduction to probability statistics.
•Bayesian Machine Learning
